{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchmetrics.classification import MultilabelAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: load data\n",
    "data_folder = \"d:/Chris/CISC881/A4/assignment4_data\"\n",
    "desi_rbl_data = np.load(os.path.join(data_folder, \"2017 09 29 DESI 08 RBL R.npz\"))\n",
    "desi_rml_data = np.load(os.path.join(data_folder, \"2017 10 03 DESI 04 RML L.npz\"))\n",
    "desi_ra_data = np.load(os.path.join(data_folder, \"2017 10 05 DESI 02 RA R.npz\"))\n",
    "\n",
    "desi_rbl_image = desi_rbl_data[\"peaks\"]\n",
    "desi_rml_image = desi_rml_data[\"peaks\"]\n",
    "desi_ra_image = desi_ra_data[\"peaks\"]\n",
    "\n",
    "desi_rbl_x, desi_rbl_y = desi_rbl_data[\"dim_x\"], desi_rbl_data[\"dim_y\"]\n",
    "desi_rml_x, desi_rml_y = desi_rml_data[\"dim_x\"], desi_rml_data[\"dim_y\"]\n",
    "desi_ra_x, desi_ra_y = desi_ra_data[\"dim_x\"], desi_ra_data[\"dim_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tic_normalization(desi_image):\n",
    "    tic = np.sum(desi_image, axis=-1, keepdims=True)\n",
    "    normalized_image = desi_image / tic\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: TIC normalization\n",
    "normalized_rbl_image = tic_normalization(desi_rbl_image)\n",
    "normalized_rml_image = tic_normalization(desi_rml_image)\n",
    "normalized_ra_image = tic_normalization(desi_ra_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(desi_image):\n",
    "    pca = PCA(n_components=3)\n",
    "    transformed_image = pca.fit_transform(desi_image)\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: visualize data\n",
    "# PCA to reduce dimensions to (N_allpixels, 3)\n",
    "rbl_pca = apply_pca(normalized_rbl_image)\n",
    "rml_pca = apply_pca(normalized_rml_image)\n",
    "ra_pca = apply_pca(normalized_ra_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to RGB (dim_x, dim_y, 3)\n",
    "rbl_spatial = rbl_pca.reshape((desi_rbl_y, desi_rbl_x, -1))\n",
    "rml_spatial = rml_pca.reshape((desi_rml_y, desi_rml_x, -1))\n",
    "ra_spatial = ra_pca.reshape((desi_ra_y, desi_ra_x, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_image_by_channel(desi_image):\n",
    "    data_min = np.min(desi_image, axis=(0, 1), keepdims=True)\n",
    "    data_max = np.max(desi_image, axis=(0, 1), keepdims=True)\n",
    "    scaled_image = (desi_image - data_min) / (data_max - data_min)\n",
    "    return scaled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale each channel to [0,1]\n",
    "rbl_spatial = scale_image_by_channel(rbl_spatial)\n",
    "rml_spatial = scale_image_by_channel(rml_spatial)\n",
    "ra_spatial = scale_image_by_channel(ra_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save images\n",
    "# plt.imsave(os.path.join(data_folder, \"rbl_spatial.jpeg\"), rbl_spatial)\n",
    "# plt.imsave(os.path.join(data_folder, \"rml_spatial.jpeg\"), rml_spatial)\n",
    "# plt.imsave(os.path.join(data_folder, \"ra_spatial.jpeg\"), ra_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6: dataset generation\n",
    "# Load roi images\n",
    "rbl_roi = sitk.ReadImage(os.path.join(data_folder, \"rbl_roi.nrrd\"))\n",
    "rml_roi = sitk.ReadImage(os.path.join(data_folder, \"rml_roi.nrrd\"))\n",
    "ra_roi = sitk.ReadImage(os.path.join(data_folder, \"ra_roi.nrrd\"))\n",
    "rbl_roi_arr = sitk.GetArrayFromImage(rbl_roi)[0]  # take first channel because mask is 2D\n",
    "rml_roi_arr = sitk.GetArrayFromImage(rml_roi)[0]\n",
    "ra_roi_arr = sitk.GetArrayFromImage(ra_roi)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_pixel_coordinates(roi_image, label):\n",
    "    i, j = np.where(roi_image == label)\n",
    "    return list(zip(i, j))  # convert list of x and y coordinates to list of (x, y) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pixels from DESI image for each class in ROI (1 = cancer, 2 = normal, 3 = background)\n",
    "rbl_cancer_pixels = get_class_pixel_coordinates(rbl_roi_arr, 1)\n",
    "rbl_normal_pixels = get_class_pixel_coordinates(rbl_roi_arr, 2)\n",
    "rbl_background_pixels = get_class_pixel_coordinates(rbl_roi_arr, 3)\n",
    "\n",
    "rml_cancer_pixels = get_class_pixel_coordinates(rml_roi_arr, 1)\n",
    "rml_normal_pixels = get_class_pixel_coordinates(rml_roi_arr, 2)\n",
    "rml_background_pixels = get_class_pixel_coordinates(rml_roi_arr, 3)\n",
    "\n",
    "ra_cancer_pixels = get_class_pixel_coordinates(ra_roi_arr, 1)\n",
    "ra_normal_pixels = get_class_pixel_coordinates(ra_roi_arr, 2)\n",
    "ra_background_pixels = get_class_pixel_coordinates(ra_roi_arr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_spectra_and_labels(desi_image, desi_x, desi_y, pixel_coordinates, label):\n",
    "    # First reshape image to (dimy, dimx, -1)\n",
    "    desi_image_reshaped = desi_image.reshape((desi_y, desi_x, -1))\n",
    "\n",
    "    # Get spectra and labels for each pixel\n",
    "    spectra = []\n",
    "    labels = []\n",
    "    for i, j in pixel_coordinates:\n",
    "        spectra.append(desi_image_reshaped[i, j, :])\n",
    "        labels.append(label)\n",
    "\n",
    "    return spectra, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spectrum and label datasets\n",
    "cancer_label = np.array([0, 0, 1])\n",
    "normal_label = np.array([0, 1, 0])\n",
    "background_label = np.array([1, 0, 0])\n",
    "\n",
    "# Training\n",
    "rbl_cancer_spectra, rbl_cancer_labels = get_class_spectra_and_labels(normalized_rbl_image, desi_rbl_x, desi_rbl_y, rbl_cancer_pixels, cancer_label)\n",
    "rbl_normal_spectra, rbl_normal_labels = get_class_spectra_and_labels(normalized_rbl_image, desi_rbl_x, desi_rbl_y, rbl_normal_pixels, normal_label)\n",
    "rbl_background_spectra, rbl_background_labels = get_class_spectra_and_labels(normalized_rbl_image, desi_rbl_x, desi_rbl_y, rbl_background_pixels, background_label)\n",
    "\n",
    "# Validation\n",
    "rml_cancer_spectra, rml_cancer_labels = get_class_spectra_and_labels(normalized_rml_image, desi_rml_x, desi_rml_y, rml_cancer_pixels, cancer_label)\n",
    "rml_normal_spectra, rml_normal_labels = get_class_spectra_and_labels(normalized_rml_image, desi_rml_x, desi_rml_y, rml_normal_pixels, normal_label)\n",
    "rml_background_spectra, rml_background_labels = get_class_spectra_and_labels(normalized_rml_image, desi_rml_x, desi_rml_y, rml_background_pixels, background_label)\n",
    "\n",
    "# Test\n",
    "ra_cancer_spectra, ra_cancer_labels = get_class_spectra_and_labels(normalized_ra_image, desi_ra_x, desi_ra_y, ra_cancer_pixels, cancer_label)\n",
    "ra_normal_spectra, ra_normal_labels = get_class_spectra_and_labels(normalized_ra_image, desi_ra_x, desi_ra_y, ra_normal_pixels, normal_label)\n",
    "ra_background_spectra, ra_background_labels = get_class_spectra_and_labels(normalized_ra_image, desi_ra_x, desi_ra_y, ra_background_pixels, background_label)\n",
    "\n",
    "X_train = np.array(rbl_cancer_spectra + rbl_normal_spectra + rbl_background_spectra)\n",
    "X_val = np.array(rml_cancer_spectra + rml_normal_spectra + rml_background_spectra)\n",
    "X_test = np.array(ra_cancer_spectra + ra_normal_spectra + ra_background_spectra)\n",
    "\n",
    "y_train = np.array(rbl_cancer_labels + rbl_normal_labels + rbl_background_labels)\n",
    "y_val = np.array(rml_cancer_labels + rml_normal_labels + rml_background_labels)\n",
    "y_test = np.array(ra_cancer_labels + ra_normal_labels + ra_background_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PyTorch datasets and dataloaders\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "X_val_tensor = torch.Tensor(X_val)\n",
    "X_test_tensor = torch.Tensor(X_test)\n",
    "\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "y_val_tensor = torch.Tensor(y_val)\n",
    "y_test_tensor = torch.Tensor(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 32, 3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 199, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "metric = MultilabelAccuracy(num_labels=3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchriscyyeung\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Chris\\Documents\\CISC881\\desi-pathology-segmentation\\wandb\\run-20230404_173038-cdr93t6n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chriscyyeung/cisc881-desi-classification/runs/cdr93t6n' target=\"_blank\">happy-brook-7</a></strong> to <a href='https://wandb.ai/chriscyyeung/cisc881-desi-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chriscyyeung/cisc881-desi-classification' target=\"_blank\">https://wandb.ai/chriscyyeung/cisc881-desi-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chriscyyeung/cisc881-desi-classification/runs/cdr93t6n' target=\"_blank\">https://wandb.ai/chriscyyeung/cisc881-desi-classification/runs/cdr93t6n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log experiments\n",
    "experiment = wandb.init(\n",
    "    project=\"cisc881-desi-classification\",\n",
    "    config={\"epochs\": 25, \"batch_size\": 1, \"learning_rate\": 0.001}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n",
      "Epoch 1 - Training loss: 0.991659420613872 - Validation loss: 0.8204411875967886 - Training accuracy: 0.7061752676963806 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 2 - Training loss: 0.9897703404057622 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079163789749146 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 3 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.707916259765625 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 4 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079166173934937 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 5 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079165577888489 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 6 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.707916259765625 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 7 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079163789749146 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 8 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079160213470459 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 9 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.707916796207428 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 10 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079163789749146 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 11 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.707916796207428 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 12 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079155445098877 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 13 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079157829284668 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 14 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079164981842041 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 15 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079165577888489 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 16 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079164981842041 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 17 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079161405563354 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 18 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079156637191772 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 19 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079166173934937 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 20 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079168558120728 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 21 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079170346260071 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 22 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079161405563354 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 23 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079168558120728 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 24 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079160809516907 - Validation accuracy: 0.8206698894500732\n",
      "Saved model\n",
      "Epoch 25 - Training loss: 0.9895773186934294 - Validation loss: 0.8204398669448554 - Training accuracy: 0.7079164981842041 - Validation accuracy: 0.8206698894500732\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(25):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    best_val_loss = np.inf\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += metric(outputs, labels)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_loss = 0.0\n",
    "        val_running_acc = 0.0\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "            val_running_acc += metric(outputs, labels)\n",
    "\n",
    "    if val_running_loss < best_val_loss:\n",
    "        best_val_loss = val_running_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"Saved model\")\n",
    "    \n",
    "    experiment.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": running_loss / len(train_dataloader),\n",
    "        \"val_loss\": val_running_loss / len(val_dataloader),\n",
    "        \"train_acc\": running_acc / len(train_dataloader),\n",
    "        \"val_acc\": val_running_acc / len(val_dataloader)\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Training loss: {running_loss / len(train_dataloader)} \"\n",
    "                            f\"- Validation loss: {val_running_loss / len(val_dataloader)} \"\n",
    "                            f\"- Training accuracy: {running_acc / len(train_dataloader)} \"\n",
    "                            f\"- Validation accuracy: {val_running_acc / len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8: prospective deployment\n",
    "# Load model\n",
    "best_model = CNN()\n",
    "best_model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "prediction = np.zeros((desi_ra_y, desi_ra_x, 3))\n",
    "ra_image_2d = np.reshape(normalized_ra_image, (desi_ra_y, desi_ra_x, -1))\n",
    "# Make prediction on each pixel\n",
    "for y in range(desi_ra_y):\n",
    "    for x in range(desi_ra_x):\n",
    "        spectrum = ra_image_2d[y, x, :]\n",
    "        spectrum_tensor = torch.Tensor(spectrum)\n",
    "        spectrum_tensor = spectrum_tensor.unsqueeze(0)\n",
    "        spectrum_tensor = spectrum_tensor.unsqueeze(1)\n",
    "        spectrum_tensor = spectrum_tensor.to(device)\n",
    "        output = model(spectrum_tensor).cpu().detach().numpy()[0]\n",
    "        prediction[y, x] = np.argmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot image\n",
    "plt.imshow(prediction)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
